{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 用python操作hbase\n",
    "\n",
    "使用happybase包来实现用python操作hbase\n",
    "\n",
    "happybase主页：http://happybase.readthedocs.io/en/latest/user.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## step1 安装 happybase"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "安装:\n",
    "    pip install happybase\n",
    "\n",
    "验证：\n",
    "    python -c \"import happybase\"\n",
    "\n",
    "如果没有错误则安装成功\n",
    "如果遇到thriftPy does not support generating module with path in protocol 'c'请参考：\n",
    "http://blog.csdn.net/sinolover/article/details/77714648\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## step2 使用 happybase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import happybase"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1 建立连接"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hbase_host = '192.168.1.254'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.1 普通连接 \n",
    "连接如果持续1分钟没有交互的话hbase服务端会自动断开"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = happybase.Connection(hbase_host,12345)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[b'myproject_mytable', b'mytable', b'test']\n"
     ]
    }
   ],
   "source": [
    "# 打印表格，验证连接是否成功\n",
    "print(conn.tables())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.2 连接池"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "pool = happybase.ConnectionPool(size=3,host=hbase_host)\n",
    "with pool.connection() as conn:\n",
    "    print(conn.tables())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2 操作表\n",
    "  创建表"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    },
    {
     "ename": "IOError",
     "evalue": "IOError(message=b'org.apache.hadoop.hbase.TableExistsException: mytable\\n\\tat sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\\n\\tat sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)\\n\\tat sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\\n\\tat java.lang.reflect.Constructor.newInstance(Constructor.java:423)\\n\\tat org.apache.hadoop.ipc.RemoteException.instantiateException(RemoteException.java:106)\\n\\tat org.apache.hadoop.ipc.RemoteException.unwrapRemoteException(RemoteException.java:95)\\n\\tat org.apache.hadoop.hbase.util.ForeignExceptionUtil.toIOException(ForeignExceptionUtil.java:45)\\n\\tat org.apache.hadoop.hbase.client.HBaseAdmin$ProcedureFuture.convertResult(HBaseAdmin.java:4621)\\n\\tat org.apache.hadoop.hbase.client.HBaseAdmin$ProcedureFuture.waitProcedureResult(HBaseAdmin.java:4579)\\n\\tat org.apache.hadoop.hbase.client.HBaseAdmin$ProcedureFuture.get(HBaseAdmin.java:4512)\\n\\tat org.apache.hadoop.hbase.client.HBaseAdmin.createTable(HBaseAdmin.java:677)\\n\\tat org.apache.hadoop.hbase.client.HBaseAdmin.createTable(HBaseAdmin.java:607)\\n\\tat org.apache.hadoop.hbase.thrift.ThriftServerRunner$HBaseHandler.createTable(ThriftServerRunner.java:1208)\\n\\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\\n\\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\\n\\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\\n\\tat java.lang.reflect.Method.invoke(Method.java:498)\\n\\tat org.apache.hadoop.hbase.thrift.HbaseHandlerMetricsProxy.invoke(HbaseHandlerMetricsProxy.java:67)\\n\\tat com.sun.proxy.$Proxy9.createTable(Unknown Source)\\n\\tat org.apache.hadoop.hbase.thrift.generated.Hbase$Processor$createTable.getResult(Hbase.java:4022)\\n\\tat org.apache.hadoop.hbase.thrift.generated.Hbase$Processor$createTable.getResult(Hbase.java:4006)\\n\\tat org.apache.thrift.ProcessFunction.process(ProcessFunction.java:39)\\n\\tat org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:39)\\n\\tat org.apache.hadoop.hbase.thrift.TBoundedThreadPoolServer$ClientConnnection.run(TBoundedThreadPoolServer.java:289)\\n\\tat org.apache.hadoop.hbase.thrift.CallQueue$Call.run(CallQueue.java:64)\\n\\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\\n\\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\\n\\tat java.lang.Thread.run(Thread.java:745)\\nCaused by: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hbase.TableExistsException): mytable\\n\\tat org.apache.hadoop.hbase.master.procedure.CreateTableProcedure.prepareCreate(CreateTableProcedure.java:299)\\n\\tat org.apache.hadoop.hbase.master.procedure.CreateTableProcedure.executeFromState(CreateTableProcedure.java:106)\\n\\tat org.apache.hadoop.hbase.master.procedure.CreateTableProcedure.executeFromState(CreateTableProcedure.java:58)\\n\\tat org.apache.hadoop.hbase.procedure2.StateMachineProcedure.execute(StateMachineProcedure.java:119)\\n\\tat org.apache.hadoop.hbase.procedure2.Procedure.doExecute(Procedure.java:498)\\n\\tat org.apache.hadoop.hbase.procedure2.ProcedureExecutor.execProcedure(ProcedureExecutor.java:1147)\\n\\tat org.apache.hadoop.hbase.procedure2.ProcedureExecutor.execLoop(ProcedureExecutor.java:942)\\n\\tat org.apache.hadoop.hbase.procedure2.ProcedureExecutor.execLoop(ProcedureExecutor.java:895)\\n\\tat org.apache.hadoop.hbase.procedure2.ProcedureExecutor.access$400(ProcedureExecutor.java:77)\\n\\tat org.apache.hadoop.hbase.procedure2.ProcedureExecutor$2.run(ProcedureExecutor.java:497)\\n')",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIOError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-11-20f54d3abfe3>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      6\u001b[0m                          \u001b[1;34m'cf1'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mdict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmax_versions\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m                          \u001b[1;34m'cf2'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mdict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmax_versions\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mblock_cache_enabled\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m                          \u001b[1;34m'cf3'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mdict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m                      })\n\u001b[0;32m     10\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtables\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\happybase\\connection.py\u001b[0m in \u001b[0;36mcreate_table\u001b[1;34m(self, name, families)\u001b[0m\n\u001b[0;32m    307\u001b[0m             \u001b[0mcolumn_descriptors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mColumnDescriptor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    308\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 309\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclient\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcreateTable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcolumn_descriptors\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    310\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    311\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mdelete_table\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdisable\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\thriftpy\\thrift.py\u001b[0m in \u001b[0;36m_req\u001b[1;34m(self, _api, *args, **kwargs)\u001b[0m\n\u001b[0;32m    196\u001b[0m         \u001b[1;31m# wait result only if non-oneway\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    197\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresult_cls\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"oneway\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 198\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_recv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_api\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    199\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    200\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_send\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_api\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\thriftpy\\thrift.py\u001b[0m in \u001b[0;36m_recv\u001b[1;34m(self, _api)\u001b[0m\n\u001b[0;32m    228\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mv\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__dict__\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    229\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mk\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;34m\"success\"\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mv\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 230\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mv\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    231\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    232\u001b[0m         \u001b[1;31m# no throws & not void api\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIOError\u001b[0m: IOError(message=b'org.apache.hadoop.hbase.TableExistsException: mytable\\n\\tat sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\\n\\tat sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)\\n\\tat sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\\n\\tat java.lang.reflect.Constructor.newInstance(Constructor.java:423)\\n\\tat org.apache.hadoop.ipc.RemoteException.instantiateException(RemoteException.java:106)\\n\\tat org.apache.hadoop.ipc.RemoteException.unwrapRemoteException(RemoteException.java:95)\\n\\tat org.apache.hadoop.hbase.util.ForeignExceptionUtil.toIOException(ForeignExceptionUtil.java:45)\\n\\tat org.apache.hadoop.hbase.client.HBaseAdmin$ProcedureFuture.convertResult(HBaseAdmin.java:4621)\\n\\tat org.apache.hadoop.hbase.client.HBaseAdmin$ProcedureFuture.waitProcedureResult(HBaseAdmin.java:4579)\\n\\tat org.apache.hadoop.hbase.client.HBaseAdmin$ProcedureFuture.get(HBaseAdmin.java:4512)\\n\\tat org.apache.hadoop.hbase.client.HBaseAdmin.createTable(HBaseAdmin.java:677)\\n\\tat org.apache.hadoop.hbase.client.HBaseAdmin.createTable(HBaseAdmin.java:607)\\n\\tat org.apache.hadoop.hbase.thrift.ThriftServerRunner$HBaseHandler.createTable(ThriftServerRunner.java:1208)\\n\\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\\n\\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\\n\\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\\n\\tat java.lang.reflect.Method.invoke(Method.java:498)\\n\\tat org.apache.hadoop.hbase.thrift.HbaseHandlerMetricsProxy.invoke(HbaseHandlerMetricsProxy.java:67)\\n\\tat com.sun.proxy.$Proxy9.createTable(Unknown Source)\\n\\tat org.apache.hadoop.hbase.thrift.generated.Hbase$Processor$createTable.getResult(Hbase.java:4022)\\n\\tat org.apache.hadoop.hbase.thrift.generated.Hbase$Processor$createTable.getResult(Hbase.java:4006)\\n\\tat org.apache.thrift.ProcessFunction.process(ProcessFunction.java:39)\\n\\tat org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:39)\\n\\tat org.apache.hadoop.hbase.thrift.TBoundedThreadPoolServer$ClientConnnection.run(TBoundedThreadPoolServer.java:289)\\n\\tat org.apache.hadoop.hbase.thrift.CallQueue$Call.run(CallQueue.java:64)\\n\\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\\n\\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\\n\\tat java.lang.Thread.run(Thread.java:745)\\nCaused by: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hbase.TableExistsException): mytable\\n\\tat org.apache.hadoop.hbase.master.procedure.CreateTableProcedure.prepareCreate(CreateTableProcedure.java:299)\\n\\tat org.apache.hadoop.hbase.master.procedure.CreateTableProcedure.executeFromState(CreateTableProcedure.java:106)\\n\\tat org.apache.hadoop.hbase.master.procedure.CreateTableProcedure.executeFromState(CreateTableProcedure.java:58)\\n\\tat org.apache.hadoop.hbase.procedure2.StateMachineProcedure.execute(StateMachineProcedure.java:119)\\n\\tat org.apache.hadoop.hbase.procedure2.Procedure.doExecute(Procedure.java:498)\\n\\tat org.apache.hadoop.hbase.procedure2.ProcedureExecutor.execProcedure(ProcedureExecutor.java:1147)\\n\\tat org.apache.hadoop.hbase.procedure2.ProcedureExecutor.execLoop(ProcedureExecutor.java:942)\\n\\tat org.apache.hadoop.hbase.procedure2.ProcedureExecutor.execLoop(ProcedureExecutor.java:895)\\n\\tat org.apache.hadoop.hbase.procedure2.ProcedureExecutor.access$400(ProcedureExecutor.java:77)\\n\\tat org.apache.hadoop.hbase.procedure2.ProcedureExecutor$2.run(ProcedureExecutor.java:497)\\n')"
     ]
    }
   ],
   "source": [
    "pool = happybase.ConnectionPool(size=3,host=hbase_host)\n",
    "with pool.connection() as conn:\n",
    "    print(conn.tables())\n",
    "    conn.create_table('mytable',\n",
    "                     {\n",
    "                         'cf1':dict(max_versions=10),\n",
    "                         'cf2': dict(max_versions=1, block_cache_enabled=False),\n",
    "                         'cf3': dict(),\n",
    "                     })\n",
    "    print(conn.tables())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "当多个用户建立表，正好有相同名字的表时将发生冲突，使用namespace可以解决表名冲突的问题，在连接时赋予table_prefix相应的namespace,此时创建的表面为namespace_tablename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[b'mytable']\n",
      "[b'mytable']\n"
     ]
    }
   ],
   "source": [
    "pool = happybase.ConnectionPool(size=3,host=hbase_host,table_prefix='myproject')\n",
    "with pool.connection() as conn:\n",
    "    print(conn.tables())\n",
    "    conn.create_table('mytable',\n",
    "                     {\n",
    "                         'cf1':dict(max_versions=10),\n",
    "                         'cf2': dict(max_versions=1, block_cache_enabled=False),\n",
    "                         'cf3': dict(),\n",
    "                     })\n",
    "    print(conn.tables())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3 操作数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#   此处接着上面的with下来，因此开头有退格\n",
    "    table = conn.table('mytable')\n",
    "    table.put(b'row-key',{b'cf:col1':})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "3.1 存储数据\n",
    "3.2 批处理数据\n",
    "3.3 使用自动计数器\n",
    "3.4 查询数据\n",
    "3.5 扫描表\n",
    "3.6 删除数据"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
